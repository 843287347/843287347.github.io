<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Self-Attention</title>
      <link href="2021/05/24/self-attention/"/>
      <url>2021/05/24/self-attention/</url>
      
        <content type="html"><![CDATA[<h3 id="为什么要用Self-Attention"><a href="#为什么要用Self-Attention" class="headerlink" title="为什么要用Self-Attention"></a>为什么要用Self-Attention</h3><p>以序列标注任务（Sequence Labeling）为例。为了更好的理解某个单词的词性，我们需要<strong>尽可能多的考虑到整句话的内容</strong>。例如下面这句话：</p><blockquote><p>I saw a saw</p></blockquote><p>我们要考虑整句话的内容，才能判断出第一个 <em>saw</em> 和第二个<em>saw</em>的词性。</p><p>产生一个问题：我们是否有一种方法，它可以考虑到整个句子的所有的词。</p><h3 id="Self-Attention怎么运作的"><a href="#Self-Attention怎么运作的" class="headerlink" title="Self-Attention怎么运作的"></a>Self-Attention怎么运作的</h3><p><img src="/843287347/843287347.github.io/2021/05/24/self-attention/Self-Attention/self-frame.png" alt="self-attention框架"></p><p>在图中，input时四个单词向量，通过了Self-attention后，output带有上下文信息的单词向量。然后再通过一个全连接层（Fully Connect）。可以通过不断地叠加Self-Attention 和FC层。一个比较著名的例子就是The Transformer。</p><p><img src="/843287347/843287347.github.io/2021/05/24/self-attention/05/24/self-attention/frame-2.png" alt="内部结构"></p><p>再这个图中，input是 $a^1,…,a^4$，output是$b^1,…,b^4$。</p><p>以$b^1$为例，从虚线可以看出，它考虑了input所有的词。</p><p><strong>如何考虑与其他词的关系呢</strong></p><p><img src="/843287347/843287347.github.io/2021/05/24/self-attention/frame-3.png"></p><p>其实就是要去判断两个词的相关性，其相关程度我们以$\alpha$ 来表示。</p><p>下面就是关于$\alpha$的计算，其实有很多中计算方法，我们可以称它为分数函数。其中dot-product 方法最常用，我们以它为例。</p><p><img src="/843287347/843287347.github.io/2021/05/24/self-attention/cal-1.png"></p><p>input向量分别是$a^1,…,a^4$ </p><ul><li><p>让他们乘上一个矩阵$W^q$，于是就得到了query向量$q^1,…,q^4$。</p></li><li><p>同理分别乘上一个矩阵$W^k$，得到key向量$k^1,…,k^4$。</p></li></ul><p>我们让$q^1$ 和$k^2$ 相乘，得到一个标量$a_{1,2}$，表示$a_1$对$a_2$的注意程度。同理我们可以计算出$a_{1,3} ,a_{1,4},a_{1,1}$ 。</p><p>然后我们通过Softmax函数进行归一化处理，得到$a_{1,1}^{\prime} ,…,a_{1,4}^\prime$。</p><p><img src="/843287347/843287347.github.io/2021/05/24/self-attention/cal-2.png"></p><ul><li><p>然后我们让$a^1,…,a^4$ 乘上矩阵$W^v$就可以得到value向量$v^1,…,v^4$。</p></li><li><p>最后通过加权求和，我们就得到$a^1$对应的output向量$b^1$。</p></li></ul><p>其中output向量$b^1$，它具有上下问信息，因为它是通过“注意”其他词并且加权求和得到的向量。</p><p>同理，我们可以通过上面的方法，分别求出$a^2,a^3,a^4$对应的输出$b^2,b^3,b^4$。</p><h3 id="用矩阵乘法的角度"><a href="#用矩阵乘法的角度" class="headerlink" title="用矩阵乘法的角度"></a>用矩阵乘法的角度</h3><p> 对于每一个向量$a^1,…,a^4$，我们分别计算他们的$q^i,k^i,v^i,i\in{1,..,4}$ 。</p><p><img src="/843287347/843287347.github.io/2021/05/24/self-attention/mat-1.png"></p><p>以向量$q^i$为例，我们将$W^q$乘上$a^1$得到$q^1$，乘上$a^2$得到$q^2$，以此类推。</p><p>其实我们可以将$a^1,…,a^4$ 拼成一个矩阵$I$ ，让$W^q$ 乘上$I$ 就得到矩阵$Q$。其中$Q = {q^1,q^2,q^3,q^4}$。同理也可以得到矩阵$K$ 和$V。</p><p><strong>计算注意力分数</strong></p><p>我们用$\alpha_{i,j}$ 表示$a^i$ 注意$a^j$的分数。</p><p><img src="/843287347/843287347.github.io/2021/05/24/self-attention/mat-2.png"></p><p>我们让$k^1,…,k^4$ 竖着拼成一个矩阵$K^T$，再乘上$q^1$ 就可以得到$\alpha_{1,1},…,\alpha_{1,4}$，同理乘上$q^2,q^3,q^4$。</p><p>用矩阵表示就是$A = K^T Q$</p><p>我们可以得到注意力分数矩阵$A$ ，然后通过Softmax得到所有的归一化的分数矩阵$A\prime。$</p><p><img src="/843287347/843287347.github.io/2021/05/24/self-attention/mat-3.png"></p><p><strong>加权求和得到输出向量</strong></p><p>再这里也是一样，我们让$v^1$ 乘上分数矩阵$A\prime$ 就可以得到$b^1$，同理可以得到$b^2,…,b^4$。</p><p>用矩阵表示就是$O = V A\prime$</p><p><img src="/843287347/843287347.github.io/2021/05/24/self-attention/mat-4.png"></p><p>整体流程如上，其中$W^q,W^k,W^v$ 是需要模型学习的参数，其他都是固定的。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>XLNet</title>
      <link href="2021/03/23/xlnet/"/>
      <url>2021/03/23/xlnet/</url>
      
        <content type="html"><![CDATA[<h2 id="XLnet论文介绍"><a href="#XLnet论文介绍" class="headerlink" title="XLnet论文介绍"></a>XLnet论文介绍</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>像BERT这样的模型已经取得好效果，他们都是基于预训练的<strong>双向</strong>文本和去噪自编码（DAE）结构。然而BERT忽略了被遮位置之间的依赖关系，同时还会遇到预训练与微调不一致的的情况。基于以上的优缺点，提出了XLnet，一种通用的自回归预训练方法，它具有以下特性：</p><ol><li>双向学习文本</li><li>克服了BERT中自回归形式的限制</li></ol><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>目前无监督预训练模型有两大类方法最受欢迎，第一类是<strong>自回归（AR）</strong>语言建模，第二类是<strong>自编码（AE）</strong>。</p><p><strong>自回归（AR）语言建模</strong>，一般是使用自回归模型（例如RNN、LSTM）去估计文本的概率密度分布。比如说，对于一个文本序列$x=(x_1,…,x_T)$，AR语言建模会将其分解为向前的累乘$p(X) = \prod_{t=1}^T p(x_t|X&lt;t)$ 或者向后的累成。但AR语言建模只能同时获取<strong>单方向</strong>的信息，而一些常用的下游任务往往会用到双向文本信息，这使AR模型有了一些局限性。</p><p><strong>自编码（AE）语言建模</strong>，并不会直接去估计概率密度分布，而是通过重构出损坏的数据的方式。一个鲜明的例子就是BERT模型，对于给定一个文本序列，按一定的比例将文本信息遮住，并且让模型尝试将其恢复出来。由于BERT为了恢复数据，因此它需要用到双向的文本信息，这有一定的好处。但缺点就是人为地添加了一些符号[MASK]，使得模型在预训练和微调阶段出现<strong>不一致</strong>的情况。</p><p>为了结合以上两种方法的优点以及避免他们的缺点，提出了XLNet。</p><h3 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h3><p><strong>AR：</strong></p><p><img src="https://i.loli.net/2021/03/24/hDZKRBMsLjytYGQ.png" alt="AR语言模型的目标函数"></p><p>类似于RNN，AR语言模型预测当前的$x_t$要依赖<strong>前$t-1$个文本</strong>，其中$h_{\theta}(x1:t-1)$就是表示模型保留的前$t-1$个文本的信息，最终以softmax的格式表示。</p><p><strong>AE：</strong></p><p><img src="https://i.loli.net/2021/03/24/OufNgIT487VCcPZ.png" alt="AE语言模型的目标函数"></p><p>以EERT为例，它是通过恢复被遮住的信息为目标训练模型，例如，对于一个序列$x$，$\hat{x}$表示被遮住后的序列，将$\overline{x}$表示为被遮住的信息，因此训练目标就是从$\hat{x}$中获取信息，来<strong>恢复$\overline{x}$。</strong>其中$m_t=1$时，表示$x_t$是被遮住的信息。</p><p><strong>排列语言模型</strong></p><p>来的NADE灵感，作者提出了排列语言模型目标函数。对于一个序列长度为$T$，它共有$T!$种排列情况，如果共享所有的排列顺序，很自然就可以获取到左右两侧的信息。举个例子：</p><p>对于原始序列1-&gt;2-&gt;3-&gt;4，如果此时预测位置3的信息，输入的顺序是1-&gt;4-&gt;3-&gt;2，那么根据从左往右处理，就可以获取到位置1和位置4的信息。在原始序列中，1和4分别是3的两侧的信息。</p><p>那么这里的目标函数可以表示为：</p><p><img src="https://i.loli.net/2021/03/24/tmeISZrQ4vJ5bdV.png" alt="排列语言模型的目标函数"></p><p>其中$Z_T$表示所有排列组合的集合，$X_{z&lt;t}$表示该排列组合$z$中前$t-1$个元素。</p><h3 id="双流自注意"><a href="#双流自注意" class="headerlink" title="双流自注意"></a>双流自注意</h3><p>有两点需要提一下：</p><ol><li>如果去预测$x_{z_t}$时，我们只需要获取前$t-1$个信息，和位置信息$z_t$。</li><li>如果去预测其他的$x_{z_j},j&gt;t$，我们需要前$t-1$个信息和内容信息$x_{z_t}$。</li></ol><p>因此对于不同任务，需要使用不同的信息，这里分为：</p><ol><li>内容表达$h_{\theta}(X_{z\le t })$，缩写为$h_{zt}$，它包含了上下文信息和$x_{z_t}$。</li><li>查询表达$g_{\theta}(X_{x&lt;t})$，缩写为$g_{z_t}$，它只包含了上下文信息。</li></ol><p>权重更新时，递推表达式为:</p><p><img src="https://i.loli.net/2021/03/24/Z9PGziTmgeFtcnd.png" alt="双流自注意权值计算"></p><p>对于两种自注意权重更新，Q矩阵和KV矩阵分别不同，其中上标$(m)$表示第m层的Transformer。</p><p>最终双流自注意的计算过程为：</p><p><img src="https://i.loli.net/2021/03/24/aIPDs9G3STCxJb5.png" alt="双流自注意结构"></p><p>对于排列顺序为3-&gt;2-&gt;4-&gt;1的组合，通过两个Attention Masks矩阵来控制，例如矩阵第一行表示的是位置1，在组合中它前面的位置有3，2，4，因此位置1可以获取到2，3，4的文本信息。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>BERT 详解</title>
      <link href="2021/03/20/bert-xiang-jie/"/>
      <url>2021/03/20/bert-xiang-jie/</url>
      
        <content type="html"><![CDATA[<p>[1].Devlin J, Chang M W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018.</p><h3 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h3><p>BERT（<strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from <strong>T</strong>ransformers），是一个强大的预训练模型，它是基于对无标签文本<strong>联合学习</strong>训练得到的。</p><p>目前有两种策略将预训练语言模型到下游任务中，分别是<em>feature-based</em> 和 <em>fine-tuning</em>。第一种方法，例如ELMo模型，它将预训练的表示作为额外的特征加入到模型里。第二种方法，例如GPT模型，它引入了模型的参数，然后通过简单的微调，得到好的模型。</p><p>但是以上两种方法都只使用了单向的语言模型，去学习通用的语言表示，这严重地限制了预训练表示的能力，因此提出了BERT模型。</p><p><img src="https://i.loli.net/2021/03/20/DEQYKgpAB2MCofb.png" alt="BERT的预训练和微调"></p><h3 id="预训练BERT"><a href="#预训练BERT" class="headerlink" title="预训练BERT"></a>预训练BERT</h3><p>在训练BERT的时候使用了两个无监督任务，分别是掩码语言模型（Masked LM）和下句预测（Next Sentence Prediction NSP）</p><h4 id="Masked-LM"><a href="#Masked-LM" class="headerlink" title="Masked LM"></a><strong>Masked LM</strong></h4><p>为了训练深度双向表示，通过随机的将一定比例的词进行遮盖，然后让模型去预测这个词。对于训练的数据中，随机选取15%的词，对于每一个选取的词，有80%的概率替换成[MASK] ，10%的概率不会变化，10%的概率用其他词替换。</p><h4 id="Next-Sentence-Prediction"><a href="#Next-Sentence-Prediction" class="headerlink" title="Next Sentence Prediction"></a><strong>Next Sentence Prediction</strong></h4><p>像问答系统（QA）和自然语言推理（NLI）任务，主要是要模型去理解两个句子之间的关系。因此对于输入的两个句子A和B，其中有50%的概率是A是真正的下一句，有50%的概率是随机的句子。</p><blockquote><p><strong>Input</strong> = [CLS] the man went to [MASK] store [SEP]  he bought a gallon [MASK] milk [SEP]</p><p><strong>Label</strong> = IsNext</p></blockquote><blockquote><p><strong>Input</strong> = [CLS] the man [MASK] to the stroe [SEP] penguin [MASK] are flight ## less birds [SEP]</p><p><strong>Label</strong> = NotNext</p></blockquote><h3 id="微调BERT"><a href="#微调BERT" class="headerlink" title="微调BERT"></a>微调BERT</h3><p>微调就是直接干脆地，因为Transformer的自注意机制让BERT可以对不同的下游任务进行建模，通过选择合适的输入和输出的格式。对于一些常见的任务，可以选择</p><ul><li>Sentence-Paraphrasing 句子-文章对</li><li>Hypothesis-Premise 假设-前提对</li><li>Question-Passage 问题-段落对</li><li>退化的 Text - null 对</li></ul><h4 id="BERT的模型结构"><a href="#BERT的模型结构" class="headerlink" title="BERT的模型结构"></a>BERT的模型结构</h4><p><img src="https://i.loli.net/2021/03/20/wEaXpLFoSA5rUyR.png" alt="不同预训练模型结构的差别"></p><p>我们可以看到OpenAI 的GPT模型使用的是从左向右的Transformer，ELMo使用的是级联的双向LSTMs结构。而BERT则是结合了双向的文本信息，通过使用Transformer块。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>The Transformer 详解</title>
      <link href="2021/03/20/the-transformer-xiang-jie/"/>
      <url>2021/03/20/the-transformer-xiang-jie/</url>
      
        <content type="html"><![CDATA[<h3 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h3><p>对于构建语言模型和机器翻译等任务，使用基于Seq2Seq架构的RNN、LSTM、GRU等传统方法，已经取得了非常好的效果。尽管目前使用了一系列的方法，提升了RNNs的效率，例如使用因式分解，条件计算等技巧。但是RNNs根本的的缺陷并没有解决，还一直保留着。</p><p>注意力机制（Attention mechanisms）已经是序列建模（sequence  modeling）和传导模型（transduction model）的不可或缺的部分。它可以找到输入、输出序列之间的依赖程度，而不依赖于词与词之间的距离。</p><p>在论文中，提出了Transformer模型，它抛弃了循环神经网络，而是完全依赖于注意力机制的一种模型结构。</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>目前效果好的神经序列传导模型都是基于encoder-decoder的结构。它的encoder部分是将符号序列表示$(x_1,…,x_n)$ 映射到连续的表示$z=(z_1,…,z_n)$，然后对于decoder部分，将$z$ 作为输入，最终输出符号序列$(y_1,…,y_m)$ ，在每一个时间步长都是自回归的：将$t-1$生成的输出，作为$t$的输入。</p><p>Transformer的模型结构如下所示：</p><p><img src="https://i.loli.net/2021/03/20/EZduNkxml2DQ83c.png" alt="Transformer结构"></p><h4 id="编码器和解码器"><a href="#编码器和解码器" class="headerlink" title="编码器和解码器"></a>编码器和解码器</h4><p><strong>Encoder：</strong> 这个编码器是由6个相同的层堆叠构成的，其中的每一层中包括两个子层。第一个子层是<strong>多头注意力机制</strong> ，第二个子层就是简单的<strong>全连接前馈网络</strong>。在每一个子层中都是用了<strong>残差连接（residual connections）</strong>和<strong>层正则化（layer normalization）</strong>。对于每一个子层，它的输出就相当于<br>$$<br>LayerNorm(x + SubLayer(x))<br>$$<br>同时为了方便，所有的输出维度都是$d_{model}=512$。</p><p><strong>Decoder：</strong> 和编码器一样，解码器也是由6个相同的层堆叠构成的。对于每一个层，除了有相同的子层之外，还添加了第三个子层，它主要是是用于处理Encoder输出的序列。和Encoder一样也是使用了残差连接和层正则化。不同的是，修改了第一个子层的多头注意力机制，通过<strong>掩盖</strong>的方式，让解码器不能处理后面的内容。（例如，假如预测下一个词任务是，已知的信息只能是前面已经出现过的词，因此需要将多余的信息抹去）。</p><h4 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h4><p><strong>Scaled Dot-Product Attention</strong></p><p>注意力机制可以描述成，给定一个查询、键-值对映射到一个输出向量。在这里使用了特定了“Scaled Dot-Product Attention”注意力计算方法。它的计算公式是<br>$$<br>Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V<br>$$<br>与additive attention 和dot-product 计算方法不同的是，它除以了$\sqrt{d_k}$，它可以很好地克服梯度很小的问题。</p><p><img src="https://i.loli.net/2021/03/20/tRqS1NehIHPmBn7.png" alt="Scaled Dot-Product Attention计算流程"></p><p><strong>Multi-Head Attention</strong></p><p>多头注意力，就是通过$h$次线性投影，生成$h$个注意力层并分别计算权重。其中每一个注意力层都是通过Scaled Dot-Product Attention函数计算，最终将输出结果重新拼接到一起</p><p><img src="https://i.loli.net/2021/03/20/thM1qoXaFwcLQPE.png" alt="Multi-Head Attention"></p><p>使用数学公式描述就是</p><p>$$<br>MultiHead(Q,K,V) = Concat(head_1,…,head_h)W^o<br>$$</p><p>$$<br>head_i =Attention(QW_i^Q,KW_i^K,VW_i^V)<br>$$</p><h4 id="基于位置的前馈神经网络"><a href="#基于位置的前馈神经网络" class="headerlink" title="基于位置的前馈神经网络"></a>基于位置的前馈神经网络</h4><p>除了<strong>多头自注意机制</strong> ，在编码器和解码器的每一个层都包含了前馈全连接网络。它是由两个线性变化层和ReLU函数构成。<br>$$<br>FFN(x) =max(0,xW_1 + b_1)W_2 +b_2<br>$$</p><p><strong>位置编码</strong></p><p>因为循环神经网络可以很好的处理时序信息，而使用注意力机制的时候，并不能很好的提供位置信息，因此需要给添加一个位置编码。有多种位置编码的方法，在原论文中，使用了三角函数的位置编码：<br>$$<br>PE（pos,2i） = sin(pos/10000^{2i/d_{model}})<br>$$</p><p>$$<br>PE（pos,2i+1) = cos(pos/10000^{2i/d_{model}})<br>$$</p><p>$i$是位置编码的为第$i$个维度。</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Nodejs 的安装</title>
      <link href="2021/03/01/an-zhuang-node.js/"/>
      <url>2021/03/01/an-zhuang-node.js/</url>
      
        <content type="html"><![CDATA[<p>参考：<a href="https://www.jianshu.com/p/13f45e24b1de">https://www.jianshu.com/p/13f45e24b1de</a></p><h2 id="安装Node-js-、-npm、yarn"><a href="#安装Node-js-、-npm、yarn" class="headerlink" title="安装Node.js 、 npm、yarn"></a>安装Node.js 、 npm、yarn</h2><p>1.下载node.js，自带了npm，下载地址：<a href="https://nodejs.org/en/download/">https://nodejs.org/en/download/</a></p><p><img src="https://i.loli.net/2021/01/23/iPxpoE2wrAcQWaT.png" alt="image-20210118153622211"></p><p>2.安装，一直next。记住你<strong>安装的路径</strong>，我的是D:\nodejs</p><p>3.验证是否安装成功</p><p> win + r ，打开 cmd。 </p><ul><li><p>测试node -v</p><p><img src="/843287347/843287347.github.io/2021/03/01/an-zhuang-node.js/node-v.jpg" alt="查看node版本"></p></li><li><p>测试npm -v</p></li><li><p>测试npm install express -g ，安装express包，-g 表示全局</p></li></ul><p>4.更改库的安装路径</p><p>​    找到你nodejs的安装路径，例如D:\nodejs。</p><ol><li><p>新建两个文件夹，分别是<strong>node_cache</strong>、<strong>node_global</strong>。</p><p>​    node_cache文件夹为nodejs的缓存目录</p><p>​    node_global文件夹为nodejs的全局目录</p></li></ol><p><img src="/843287347/843287347.github.io/2021/03/01/an-zhuang-node.js/cache.jpg" alt="安装路径"></p><ol start="2"><li><p>设置npm的安装路径</p><p>记住<strong>你的安装路径</strong>，我的是D:\nodejs    </p><p>打开cmd或者 windows powershell，输入</p><p><code>npm config set prefix &quot;D:\nodejs\node_global&quot;</code></p><p><code> npm config set cache &quot;D:\nodejs\node_cache&quot;</code></p></li><li><p>再次安装express，验证安装到指定目录。</p><p>用<strong>管理员</strong>身份打开，或者win + x ，打开windows powershell (管理员)</p><p>输入<code>npm install express -g</code></p><p>可以看到，安装了express包到了指定文件夹中</p></li><li><p>添加环境变量</p><p>win + r ，输入<code>sysdm.cpl</code></p><p>选择，高级，环境变量</p><p><img src="https://i.loli.net/2021/01/23/7KjIJfF5XvwazCb.png" alt="image-20210118160722979"></p></li></ol><p>选择系统变量的Path，编辑<img src="https://i.loli.net/2021/01/23/GsDacMdyeRnTOh4.png" alt="image-20210118160801663"></p><p>新建，输入路径D:\nodejs\node_global，保存并退出<img src="/843287347/843287347.github.io/2021/03/01/an-zhuang-node.js/path.jpg" alt="路径"></p><p>验证，win + x，选择 powershell 管理员。</p><p>安装yarn: <code> npm install yarn -g</code></p><p>验证：<code>yarn --version</code></p><p><img src="https://i.loli.net/2021/01/23/2B9KXbMzCJNeonc.png" alt="image-20210118161125959"></p><h2 id="用visual-studio-code-打开项目文件夹"><a href="#用visual-studio-code-打开项目文件夹" class="headerlink" title="用visual studio code 打开项目文件夹"></a>用visual studio code 打开项目文件夹</h2><p> 打开终端，输入<code>yarn</code>，安装依赖</p><p><img src="https://i.loli.net/2021/01/23/mdbO7NMuolj8fi1.png" alt="image-20210118161856121"></p><p>安装完成后，输入<code>yarn start</code></p><p>等待打包，运行程序。</p><p>进入Local 网页</p><p><img src="https://i.loli.net/2021/01/23/klQZJpzvDOCo7FT.png" alt="image-20210118162240548"></p><p>安装插件：</p><p>HTML snippets</p><p>JavaScript (ES6) code snippets</p><p>CSS Peek</p><p>Path autocomplete</p><p>Reactjs code snippets</p><p>ESLint</p>]]></content>
      
      
      <categories>
          
          <category> 前端 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nodejs </tag>
            
            <tag> npm </tag>
            
            <tag> yarn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DIALOGPT Large-Scale Generative Pre-trainingfor Conversational Response Generation</title>
      <link href="2021/02/07/lunwen-dialogpt/"/>
      <url>2021/02/07/lunwen-dialogpt/</url>
      
        <content type="html"><![CDATA[<h2 id="《DialoGPT-Large-Scale-Generative-Pre-training-for-Conversational-Response-Generation》"><a href="#《DialoGPT-Large-Scale-Generative-Pre-training-for-Conversational-Response-Generation》" class="headerlink" title="《DialoGPT : Large-Scale Generative Pre-training for Conversational Response Generation》"></a><strong>《DialoGPT : Large-Scale Generative Pre-training for Conversational Response Generation》</strong></h2><blockquote><p>该论文发表在ACL-2020，是由Microsoft团队<strong>Yizhe Zhang</strong> 、<strong>Siqi Sun</strong>等作者共同完成的。</p></blockquote><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>这篇论文提出了一个大型、可微调的神经对话响应生成模型，<strong>DialoGPT</strong>。是从Reddit中2005年至2017年的聊天对话数据中训练得来的。DialoGPT扩展了Huging Face PyTorch的transformer，使其在单论对话性能接近人类。作者表明，DialoGPT的会话系统比其他系统产生更加相关、内容更丰富以及上下文一致的响应。预先训练的模型和训练管道被公开发布，以促进神经响应生成的研究和开发更智能的开放域对话系统。</p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>基于transformer结构的大规模预训练模型已经取得了很大成功。例如OpenAI的GPT-2已经证明，在非常大的数据集上训练的transformer模型可以捕获文本数据中的长期依赖关系，并生成流畅、词汇多样和内容丰富的文本。这样的模型有能力捕获细粒度的文本数据，并以高分辨率产生输出，该高分辨率紧密模拟人类编写的真实世界文本。</p><p><strong>DialoGPT是在GPT-2的基础上拓展的</strong>，用于解决对话神经响应生成的问题。神经响应生成是文本生成的一个子类别，但是它们都是生成与提示相关的自然文本。目前大多数开放域神经响应生成系统存在一些问题，<strong>例如内容和风格不一致，缺乏长期的上下文信息和生成的响应过于平淡</strong>。</p><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>该数据集是Reddit从2005年到2017年的评论链中提取的。<strong>Reddit的讨论可以自然地扩展为树状结构的回复链</strong>，将根节点到叶节点的每条路径作为包含多个对话回合的训练实例，并提取出来。我们通过删除其中的实例来过滤数据</p><ul><li>源或目标中有一个URL。</li><li>其中目标包含至少三个单词的单词重复。</li><li>如果答复不包含至少一个最常见的50个英语单词(例如，“the”、“of”、“a”)，因为这可能表明它可能不是一个英语句子。</li><li>其中响应包含特殊标记，如“[”或“]”，因为这可能是标记语言。</li><li>其中源序列和目标序列一起超过200个单词。</li><li>其中目标包含攻击性语言，由与大块列表匹配的短语标识。</li></ul><p>经过筛选，数据集包含147,116,725个对话实例，共计<strong>18亿字</strong>。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>DialoGPT是在GPT-2结构的基础上训练的。GPT-2 的transformer模型采用的是通用的transformer语言模型，利用掩码多头自注意层所构成的栈来训前文本数据。我们的模型继承了GPT-2(Radford等人，2018年)，它由12至48层transformer并且包括normalization层，一种解释我们修改的模型深度的初始化方案，以及字节对编码用于标记器。</p><p>跟OpenAI的GPT-2一样，将多转对话会话建模为长文本，并将生成任务框架为语言建模。我们把源句子记作S，目标句子记作T，<br>$$<br>S= x_1, …,x_m ， T = x_{m+1},…,x_N<br>$$<br>其中N是序列长度，因此条件概率P(T|S)可以分解为：<br>$$<br>p(T|S) = \prod_{n=m+1}^N p(x_n | x_1,…,x_{n-1})<br>$$<br>多轮对话T1,…,Tk，上式也可以写作$p(T_k,…,T_2 | T_1)$ ，它本质上是条件概率$P(T_i | T_1,…,T_{i-1})$的累乘。因此优化单个$p(T_k,…,T_2 | T_1)$ ，可以视为优化所有的$P(T_i | T_1,…,T_{i-1})$对。</p><h3 id="最大互信息"><a href="#最大互信息" class="headerlink" title="最大互信息"></a>最大互信息</h3><p>开放域文本生成模型因生成平淡无奇的、缺乏信息的样本而臭名昭著。 为了解决这个问题，实现了一个最大互信息（<strong>MMI</strong>）评分函数。<strong>MMI</strong>采用一个预先训练的反向模型来预测给定响应的源句子，即P（source|target）。</p><p>首先使用top-K抽样生成一组假设。 然后利用P（source|Hypothesis）的概率对所有假设进行重新排序。 直觉上，最大限度地向后模型似然会惩罚平淡的假设。因为重复出现的假设可能与一些查询有关联，而对其他特定查询关联的概率很低。</p><p>本文还试图使用policy梯度与样本平均基线来优化奖励R=P（source|Hypothesis）。好的奖励可以稳定地让模型提高，但与RNN体系结构下的训练不同，我们观察到强化学习(RL)训练很容易收敛到退化的局部最优解。</p><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><h3 id="实验细节"><a href="#实验细节" class="headerlink" title="实验细节"></a>实验细节</h3><p>训练了三个不同大小的模型，他们的参数分别是117M，345M和762M。模型使用了一个包含50，257个条目的词典，并且在16个Nvidia V100机器上训练，使用了NVLink。使用了Noam学习速率调度器和16000个热身步骤。学习率根据验证损失选择。 每个模型都被训练，直到验证损失没有更新。 对于中小型模型，我们训练了多达5个epochs的模型。 训练的大模型3个epochs。</p><p><strong>训练加速</strong>：将训练数据压缩至lazy-loading 数据库文件。还利用单独的异步数据进程来扩展训练。除此之外，进一步使用动态批处理策略将长度相似的会话分组到同一批中，从而提高了训练吞吐量。</p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>基于DSTC的评价数据</p><p><img src="https://i.loli.net/2021/02/07/czn2A4L1tlVXrmI.png" alt="image-20210207163620461"></p><hr><p>基于6K Reddit multi-reference 评价数据</p><p><img src="https://i.loli.net/2021/02/07/DcXbMo7rxYpLR8w.png" alt="image-20210207163630238"></p><p>在表格3中，相较于Greedy 生成，MMI重排后会产生更加多样的响应。具有更好的NIST、METEOR、Entropy和DIst分数，但是BLEU分有所下降。</p><h3 id="生成的例子"><a href="#生成的例子" class="headerlink" title="生成的例子"></a>生成的例子</h3><p><img src="https://i.loli.net/2021/02/07/jlJgSbKiH9ZUd3W.png" alt="image-20210207164127603"></p><p><img src="https://i.loli.net/2021/02/07/XD2kNIQfsTrRS56.png" alt="image-20210207164133258"></p><p><img src="https://i.loli.net/2021/02/07/J9BwutckgbqW6Zs.png" alt="image-20210207164234170"></p><h2 id="局限性和风险"><a href="#局限性和风险" class="headerlink" title="局限性和风险"></a>局限性和风险</h2><p>尽管我们努力在训练前减少具有攻击性的数据，但DialoGPT依然有可能会产生不好的回应。输出的数据中隐含的性别偏见和其他历史偏见。发布DialoGPT的一个主要动机是使研究人员能够调查这些问题并制定缓解策略。 在任何情况下都不应因使用DialoGPT而产生不适当的内容被解释为反映作者或微软公司的观点或价值观。</p><p><strong>参考文献：</strong></p><ol><li>Zhang Y, Sun S, Galley M, et al. Dialogpt: Large-scale generative pre-training for conversational response generation[J]. arXiv preprint arXiv:1911.00536, 2019.</li></ol>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Towards a Human-like Open-Domain Chatbot</title>
      <link href="2021/01/31/lunwen-meena/"/>
      <url>2021/01/31/lunwen-meena/</url>
      
        <content type="html"><![CDATA[<h2 id="《Towards-a-Human-like-Open-Domain-Chatbot》"><a href="#《Towards-a-Human-like-Open-Domain-Chatbot》" class="headerlink" title="《Towards a Human-like Open-Domain Chatbot》"></a>《<strong>Towards a Human-like Open-Domain Chatbot</strong>》</h2><blockquote><p>该论文是由google的<strong>Daniel Adiwardana</strong>、<strong>Minh-Thang Luong</strong>等作者合作完成的。发表在ACL-2020会议上。</p></blockquote><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>在这篇论文中，提出了一个多轮、开放域聊天机器人 <strong>Meena</strong>，它是挖掘和过滤后的公众社交媒体的数据端到端训练完成的。它拥有2.6B个参数的神经网络模型，训练目标仅是简单的求最小困惑度度。同时该论文也提出了人类评价指标SSA（Sensibleness and Specificity Average），这个指标能够捕获和人类相似的多轮聊天中的关键信息。通过实验，我们看到困惑度（perplexity）和SSA有很强的相关性。一个事实就是，通过端到端训练后拥有最低困惑度的<strong>Meena</strong>在SSA上取得很高的分数，这表明，如果我们能更好地优化困惑度，86%的SSA是可以达到的。此外，完整版本的Meena（具有过滤机制和调谐解码）得分79%的SSA，我们估计23%的绝对SSA高于现有的聊天机器人。</p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>能够用自然语言进行自由交谈是人类智慧的标志之一，同时也是对真正的人工智能的要求。为了探索真正的智能这一方面，许多研究人员正在研究开放域聊天机器人。目前一些开放域的聊天机器人例如<strong>MILABOT</strong>、<strong>XiaoIce</strong>、<strong>Mitsuku</strong>和<strong>Cleaverbot</strong> ，它们都展示出了人的一些属性，但是依赖于复杂框架，例如基于知识图谱、基于检索和基于规则的系统。尽管进行了大量的研究，但开放域聊天机器人仍然有一些弱点，使它们无法普遍适用，它们往往会给出无意义、模糊的回答。</p><p>该文介绍了Meena，一种生成性聊天机器人模型，它是在公共领域社交媒体对话中挖掘和过滤的40B单词上进行端到端训练的。对于Meena，他推动了端到端方法的限制，并表明大规模的低复杂性模型可以是一个很好的会话者。它将基于ET（Evolved Transformer）的Seq2Seq的模型作为主要结构。该模型是在多回合对话中训练的，其中输入序列是上下文的所有回合（最多7），输出序列是回答。我们的最佳模型有2.6B参数，并且基于8k个BPE子单词的词汇表实现了10.2的困惑度。</p><p>为了测量Meena和其他聊天机器人的质量，提出了一个简单的人类评价度量。敏感性和具体性平均 <strong>SSA</strong> 结合了一个类似人类的聊天机器人的两个基本方面：有意义和具体性。我们要求人类评委对每一个回答在这两个方面进行打分。这里提供了两种评估方式，一种是静态评估，另一种是交互式评估。静态评估就是给定多轮对话的数据集，而交互式评估就是人类评委能够进行随意交流。</p><p>该文主要的贡献是：</p><ul><li>提出了一个简单的人类评估指标，可以用于开放域多轮对话模型。</li><li>表明困惑是一种与人类判断相关的自动度量</li><li>证明了一个具有足够低困惑的端到端神经模型可以超越现有聊天机器人的敏感性和特异性</li></ul><h2 id="对聊天机器人的评估"><a href="#对聊天机器人的评估" class="headerlink" title="对聊天机器人的评估"></a>对聊天机器人的评估</h2><h3 id="对聊天机器人的评估-1"><a href="#对聊天机器人的评估-1" class="headerlink" title="对聊天机器人的评估"></a>对聊天机器人的评估</h3><p>为了评估模型回应的质量，我们提出了两个问题的序列。首先判断回应是否有意义，因为有意义代表了基础的常识知识和逻辑连贯性。回应仅仅有意义的还不够，会进一步询问被测试人员是否回答准确。例如：A说“我喜欢打网球”，B说“真好”，这样的回答并不是准确的。如果B说：“我也是，我特别喜欢罗杰·费德勒”，这样的回答是准确的。</p><p>为了将这两个指标组合成一个度量，我们取两个指标中的平均值，我们称之为SSA（敏感性和特异性平均值）。SSA能够代表人类相似度，它会惩罚经常产生通用响应的聊天机器人。在SSA出现之前，经常有两个问题困扰着研究者们，第一个就是人工测试的工作人员需要做什么，第二个就是工作人员如何更好地表达。SSA能够解决这两个问题。首先，SSA很容易被工作人员理解，其次，附加的问题没有额外信息，最后能够很多主观问题会被筛选掉。</p><h3 id="静态评估"><a href="#静态评估" class="headerlink" title="静态评估"></a>静态评估</h3><p>为了有一个通用的比较模型基准，我们创建了一个包含1477个会话上下文的集合，在1到3个会话回合之间，我们称之为Mini-图灵基准(MTB)。MTB还包含有个性问题的上下文(例如。 “你喜欢猫吗？”)，其中一些人期待着有个性一致性的回应。例如：</p><pre class="line-numbers language-none"><code class="language-none">A：“你喜欢看电影嘛”，B：“喜欢，我最喜欢的就是科幻电影”，A：“你最喜欢哪一部？”<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>期待的回答应该类似于“我喜欢《I Love Back to Future》”，相反如果回答“我不喜欢电影”，那么就会自相矛盾，因此会被视为无意义的回应。</p><p>在评估聊天机器人时，我们将MTB文本喂给模型或者给人类，以获取响应。 我们将得到的（context，response）对给工作者，并询问是否一个response是明智和具体的。 我们称之为静态评估，这是因为上下文是固定的。</p><h3 id="交互式评估"><a href="#交互式评估" class="headerlink" title="交互式评估"></a>交互式评估</h3><p>静态评估可能适合于比较模型，对静态评估数据集的构造往往有偏见。为了解决这一问题，我们创建了一个额外的评估模式，在这种模式下，工作人员可以与聊天机器人1：1聊天，讨论他们想要的任何东西。和静态评估一样，还需要工作人员去判断聊天机器人给的回应是否有意义并且准确。与典型的图灵测试不同，我们预先告诉人类评委，他们即将与一个实验的聊天机器人聊天，并要求他们在感知和特异性方面标记聊天机器人说的话。</p><h2 id="Meena-聊天机器人"><a href="#Meena-聊天机器人" class="headerlink" title="Meena 聊天机器人"></a>Meena 聊天机器人</h2><p>如上所述，最近关于端到端对话模型的工作分为两大类：第一类是具有人工设计组件的复杂模型，另一类是大型神经网络模型（称为端到端模型），后者更接近通用学习框架。一个悬而未决的问题是：为了达到更好的模型质量，简单的扩大模型可以实现吗？比如增加训练数据量和增加模型参数数量。亦或者是将这样的模型与其他组件结合起来？在本节中，我们将介绍Meena模型，他是目前为止最大的端到端模型。我们相信它回答了刚才的问题，表明一个大型的端到端模型可以在开放域设置中生成几乎类似人类的聊天响应。</p><h3 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h3><p>用于训练Meena的数据集是从公共领域社交媒体对话中挖掘和过滤的。源数据本质上是一棵消息树，包含了多名对话人员。其中第一个消息是根节点，他的孩子节点都是对应一个消息。沿着树的任何路径都展示了一次对话，其中每条消息都是一个对话回合。通过将会话路径中的每个回合作为响应处理，并将以前的所有回合（最多7)作为上下文处理，我们创建了(context、response）表单对的训练示例。</p><p>同时我们也对数据进行了过滤，当满足删除条件时，会将该对话的子树移除。过滤之后的数据大约867M的个(context、response）对。文本是通过BPE（byte-pair-encoding）进行分词。我们使用8K个BPE子单词的词汇。最终，Meena 数据集大概341GB的文本（40B 词）。相比较，GPT-2的数据集为40GB。</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>性能最好的Meena模型是Evolved Transformer(ET)seq2seq模型，具有2.6B参数，包括1个ET编码块和13个ET解码块。ET是NAS结构演化来的。我们最大的（即最大内存使用）ET得分10.2困惑度，我们最大的Vanilla  Transformer得分10.7，使用相同数量的训练步骤(738K)。最大的Vanilla Transformer有32个解码器层与其他架构超参数保持不变。</p><p>作为比较，超大型GPT-2模型(Radford等人，2019年)具有1.5B参数，是一种语言模型（即仅解码器）；而最近DialoGPTwo的大型会话模型 雷克(Zhang等人，2019年)有762M参数。</p><p>Meena的隐藏层有2560个节点，注意头数是32。 我们在编码器、解码器和Softmax层共享词嵌入。 编码器和解码器的最大长度是 128个字符（即256个组合）。 我们的最佳模型的超参数是通过手动坐标下降搜索找到的。</p><h3 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h3><p>最佳的模型用TPU-v3 Pod (2048个TPU核) 在Meena数据集上运行了30天，该数据集上包含40B个单词。另外有趣的是，这个2.6B参数的模型会在61B单词的数据集上过拟合，这表明模型容量惊人的大。因此我们添加一个0.1的注意力和前馈层的dorpout。此外，为了节省内存，我们选择了Adafactor优化器，以0.01为初始学习速率，在前10k步骤中保持不变，然后用步骤数的平方根的倒数衰减。</p><h3 id="解码"><a href="#解码" class="headerlink" title="解码"></a>解码</h3><p>聊天机器人往往会生成无聊的，通用的回答。为了解决这个问题，往往会使用复杂的解码算法，例如不同形式的重排列或者调整配置文件，主题和风格。最近研究工作也探索了新的框架，例如对抗学习，变分自动编码或者两者的结合。</p><p>我们表明，如果提供要给足够低困惑底的模型，一个简单的 “采样-排列” 的解码策略就可以实现了多样化和高质量的回答。 工作如下 ，首先我们使用温度为T的普通随机抽样，采样N个独立的备选回答，然后我们选择概率最高的候选回答作为最终输出。温度T&gt;0是一个超参数，它在解码过程中调节下一个令牌的概率分布pi。<br>$$<br>p_i = \frac{exp(\frac{z_i}{T})}{\sum_j exp(\frac{z_j}{T} )}<br>$$<br>我们观察到，T的越大越有利于上下文罕见的词，同时，较小的T值有利于更常见的词，如冠词或介词，它们更保守，但不那么具体。</p><p><img src="https://i.loli.net/2021/02/03/kNypxVQlMeOzb83.png" alt="image-20210203143701552"></p><p><img src="https://i.loli.net/2021/02/03/vsHaj6KJZyOoYhz.png" alt="image-20210203143720463"></p><p>从表2 和表3 中我们可以看出来相对于Beam Search，采样再排序提供更多样性的回答。</p><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><h3 id="SSA-和-困惑度的相关性"><a href="#SSA-和-困惑度的相关性" class="headerlink" title="SSA 和 困惑度的相关性"></a>SSA 和 困惑度的相关性</h3><p>静态评估的意义性与困惑的相关性R2=0.93，静态特异性与困惑的相关性R2=0.94，表明这可能是一个很好的自动测量意义性和准确性的指标。 静态SSA与困惑度有R2=0.94。 静态评价结果如图5所示。 相关性接近线性，但尚不清楚这一趋势是否会达到更低的困惑度。</p><p><img src="https://i.loli.net/2021/02/03/BeMId8HFSLwGA5y.png" alt="image-20210203144417790"></p><p>在交互式评估（第2.3节）中，工作人员可以谈论他们想要的任何东西。 我们观察到同样强的与困惑度的相关性（见图1、图3和图4）。 这表明与困惑度的静态评估相关性不是由于数据集的偏差。</p><p>在一致性方面，静态评估对最低困惑度模型进行了7次评估，交互式评估对最低困惑度模型进行了7次评估。每次我们都得到一组不同的随机采样 领导的反应。 在整个评估中，静态SSA的标准差为2%，交互式SSA的标准差为1%，这表明这两个指标对于我们的目的来说是足够一致的。</p><h3 id="模型比较"><a href="#模型比较" class="headerlink" title="模型比较"></a>模型比较</h3><p>我们获取到其他的聊天机器人的模型，并通过公司的志愿者对不同的聊天机器人进行评估，下面展示几个简单的例子。</p><p><img src="https://i.loli.net/2021/02/03/IEtBuz6Mcva3GAl.png" alt="image-20210203145011922"></p><p>更多的例子，可以查看原论文。</p><h2 id="SSA的改进"><a href="#SSA的改进" class="headerlink" title="SSA的改进"></a>SSA的改进</h2><ul><li>使用更好的解码算法</li><li>解决交叉重复的问题</li><li>设置安全层</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>git使用汇总</title>
      <link href="2021/01/29/git-shi-yong-hui-zong/"/>
      <url>2021/01/29/git-shi-yong-hui-zong/</url>
      
        <content type="html"><![CDATA[<h2 id="git汇总"><a href="#git汇总" class="headerlink" title="git汇总"></a>git汇总</h2><p>不用老忘，好记性不如烂笔头（指键盘）。</p><h3 id="如何创建一个新的github仓库"><a href="#如何创建一个新的github仓库" class="headerlink" title="如何创建一个新的github仓库"></a>如何创建一个新的github仓库</h3><ol><li>一般都是先从github 或者gitte中新建好仓库（respository)。</li><li>创建好仓库之后，这样就可以获取到仓库的链接（点击克隆/下载）。</li><li>在你的电脑上新建一个文件夹，它是准备用来保存你项目代码的。</li><li>右键这个文件夹，选择 <code>git bash</code> 。</li><li>然后<code>git clone &quot;你的github 仓库的地址&quot;</code></li><li>这样，你的本地仓库（文件夹）和 github 的仓库就绑定好了。</li><li>把你的代码，复制到文件夹内，然后在bash里面<code>git push</code>。</li></ol>]]></content>
      
      
      <categories>
          
          <category> git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="2021/01/22/hello-world/"/>
      <url>2021/01/22/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
