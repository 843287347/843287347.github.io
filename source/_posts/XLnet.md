---
title: XLNet
top: false
cover: false
toc: true
mathjax: true
date: 2021-03-23 21:36:50
password:
summary:
tags:
categories:
---

## XLnet论文介绍

### 简介

像BERT这样的模型已经取得好效果，他们都是基于预训练的**双向**文本和去噪自编码（DAE）结构。然而BERT忽略了被遮位置之间的依赖关系，同时还会遇到预训练与微调不一致的的情况。基于以上的优缺点，提出了XLnet，一种通用的自回归预训练方法，它具有以下特性：

1. 双向学习文本
2. 克服了BERT中自回归形式的限制

### 背景

目前无监督预训练模型有两大类方法最受欢迎，第一类是**自回归（AR）**语言建模，第二类是**自编码（AE）**。

**自回归（AR）语言建模**，一般是使用自回归模型（例如RNN、LSTM）去估计文本的概率密度分布。比如说，对于一个文本序列$x=(x_1,...,x_T)$，AR语言建模会将其分解为向前的累乘$p(X) = \prod_{t=1}^T p(x_t|X<t)$ 或者向后的累成。但AR语言建模只能同时获取**单方向**的信息，而一些常用的下游任务往往会用到双向文本信息，这使AR模型有了一些局限性。

**自编码（AE）语言建模**，并不会直接去估计概率密度分布，而是通过重构出损坏的数据的方式。一个鲜明的例子就是BERT模型，对于给定一个文本序列，按一定的比例将文本信息遮住，并且让模型尝试将其恢复出来。由于BERT为了恢复数据，因此它需要用到双向的文本信息，这有一定的好处。但缺点就是人为地添加了一些符号[MASK]，使得模型在预训练和微调阶段出现**不一致**的情况。

为了结合以上两种方法的优点以及避免他们的缺点，提出了XLNet。

### 目标函数

**AR：**

![AR语言模型的目标函数](https://i.loli.net/2021/03/24/hDZKRBMsLjytYGQ.png)

类似于RNN，AR语言模型预测当前的$x_t$要依赖**前$t-1$个文本**，其中$h_{\theta}(x1:t-1)$就是表示模型保留的前$t-1$个文本的信息，最终以softmax的格式表示。

**AE：**

![AE语言模型的目标函数](https://i.loli.net/2021/03/24/OufNgIT487VCcPZ.png)

以EERT为例，它是通过恢复被遮住的信息为目标训练模型，例如，对于一个序列$x$，$\hat{x}$表示被遮住后的序列，将$\overline{x}$表示为被遮住的信息，因此训练目标就是从$\hat{x}$中获取信息，来**恢复$\overline{x}$。**其中$m_t=1$时，表示$x_t$是被遮住的信息。

**排列语言模型**

来的NADE灵感，作者提出了排列语言模型目标函数。对于一个序列长度为$T$，它共有$T!$种排列情况，如果共享所有的排列顺序，很自然就可以获取到左右两侧的信息。举个例子：

对于原始序列1->2->3->4，如果此时预测位置3的信息，输入的顺序是1->4->3->2，那么根据从左往右处理，就可以获取到位置1和位置4的信息。在原始序列中，1和4分别是3的两侧的信息。

那么这里的目标函数可以表示为：

![排列语言模型的目标函数](https://i.loli.net/2021/03/24/tmeISZrQ4vJ5bdV.png)

其中$Z_T$表示所有排列组合的集合，$X_{z<t}$表示该排列组合$z$中前$t-1$个元素。

### 双流自注意

有两点需要提一下：

1. 如果去预测$x_{z_t}$时，我们只需要获取前$t-1$个信息，和位置信息$z_t$。
2. 如果去预测其他的$x_{z_j},j>t$，我们需要前$t-1$个信息和内容信息$x_{z_t}$。

因此对于不同任务，需要使用不同的信息，这里分为：

1. 内容表达$h_{\theta}(X_{z\le t })$，缩写为$h_{zt}$，它包含了上下文信息和$x_{z_t}$。
2. 查询表达$g_{\theta}(X_{x<t})$，缩写为$g_{z_t}$，它只包含了上下文信息。

权重更新时，递推表达式为:

![双流自注意权值计算](https://i.loli.net/2021/03/24/Z9PGziTmgeFtcnd.png)

对于两种自注意权重更新，Q矩阵和KV矩阵分别不同，其中上标$(m)$表示第m层的Transformer。

最终双流自注意的计算过程为：

![双流自注意结构](https://i.loli.net/2021/03/24/aIPDs9G3STCxJb5.png)

对于排列顺序为3->2->4->1的组合，通过两个Attention Masks矩阵来控制，例如矩阵第一行表示的是位置1，在组合中它前面的位置有3，2，4，因此位置1可以获取到2，3，4的文本信息。